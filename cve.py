# -*- coding: utf-8 -*-
"""CVE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1we8jl4MKqHIz0uSCYMN_Qv1qiujJpGAt
"""

!pip install -q pandas numpy scikit-learn joblib regex python-dateutil sentence-transformers

import os, json, html, unicodedata
import numpy as np
import pandas as pd
import regex as re2
import re
from pathlib import Path
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import KFold, train_test_split
from google.colab import files

up = files.upload()
DATA_PATH = "/content/" + next(iter(up.keys()))

CONFIG = {
    "DATA_PATH": DATA_PATH,
    "OUTPUT_DIR": "/content/prepared",
    "COLS": {"id": "ID", "text": "Description", "cvss": "cvss_vector", "label": "Exploitability"},
    "DROP_IF_BOTH_EMPTY": True,
    "ALLOW_CVSS_INVALID": True,
    "SAVE_CLASS_WEIGHTS": True
}
OUT_DIR = Path(CONFIG["OUTPUT_DIR"])
OUT_DIR.mkdir(parents=True, exist_ok=True)

AV_map_b = {"NETWORK":"N", "ADJACENT":"A", "LOCAL":"L", "PHYSICAL":"P"}
AC_map_b = {"LOW":"L", "HIGH":"H"}
PR_map_b = {"NONE":"N", "LOW":"L", "HIGH":"H"}
UI_map_b = {"NONE":"N", "REQUIRED":"R"}
S_map_b  = {"UNCHANGED":"U", "CHANGED":"C"}
IM_map_b = {"NONE":"N", "LOW":"L", "HIGH":"H"}

def _norm(s):
    return str(s).strip().upper() if not pd.isna(s) else ""

def _map(v, m):
    v = _norm(v)
    return m[v] if v in m else None

def build_cvss_row(r):
    AV = _map(r.get("AttackVector", ""), AV_map_b)
    AC = _map(r.get("AttackComplexity", ""), AC_map_b)
    PR = _map(r.get("PrivilegesRequired", ""), PR_map_b)
    UI = _map(r.get("UserInteraction", ""), UI_map_b)
    S  = _map(r.get("Scope", ""), S_map_b)
    C  = _map(r.get("ConfidentialityImpact", ""), IM_map_b)
    I  = _map(r.get("IntegrityImpact", ""), IM_map_b)
    A  = _map(r.get("AvailabilityImpact", ""), IM_map_b)
    if None in (AV,AC,PR,UI,S,C,I,A):
        return ""
    return f"AV:{AV}/AC:{AC}/PR:{PR}/UI:{UI}/S:{S}/C:{C}/I:{I}/A:{A}"

def _clean_text_basic(s: str) -> str:
    if pd.isna(s):
        return ""
    s = html.unescape(str(s))
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _standardize_cvss_vector(s: str) -> str:
    if not isinstance(s, str) or not s.strip():
        return ""
    s = s.upper().replace(" ", "")
    pairs = dict(re2.findall(r"(AV|AC|PR|UI|S|C|I|A):([A-Z])", s))
    keys = ["AV","AC","PR","UI","S","C","I","A"]
    if not pairs:
        return ""
    return "/".join([f"{k}:{pairs[k]}" for k in keys if k in pairs])

def _to01(v):
    if pd.isna(v):
        return np.nan
    s = str(v).strip().lower()
    if s in {"1","true","yes","y","t"}: return 1
    if s in {"0","false","no","n","f"}: return 0
    try:
        return int(float(s))
    except:
        return np.nan

def deduplicate(df: pd.DataFrame, id_col: str, text_col: str, cvss_col: str) -> pd.DataFrame:
    df2 = df.copy()
    if id_col in df2.columns:
        df2 = df2.drop_duplicates(subset=[id_col], keep="first")
    else:
        df2 = df2.drop_duplicates(subset=[text_col, cvss_col], keep="first")
    return df2.reset_index(drop=True)

def compute_and_save_class_weights(y, out_dir: str):
    classes = np.unique(y)
    cw = compute_class_weight(class_weight="balanced", classes=classes, y=y)
    weights = {int(c): float(w) for c, w in zip(classes, cw)}
    with open(str(Path(out_dir) / "class_weights.json"), "w") as f:
        json.dump(weights, f, indent=2)

raw = pd.read_csv(CONFIG["DATA_PATH"])
raw[CONFIG["COLS"]["cvss"]] = raw.apply(build_cvss_row, axis=1)
raw = deduplicate(raw, CONFIG["COLS"]["id"], CONFIG["COLS"]["text"], CONFIG["COLS"]["cvss"])
if CONFIG["COLS"]["id"] not in raw.columns or raw[CONFIG["COLS"]["id"]].isna().all():
    raw[CONFIG["COLS"]["id"]] = [f"row_{i}" for i in range(len(raw))]
raw["text_clean"] = raw[CONFIG["COLS"]["text"]].fillna("").apply(_clean_text_basic)
raw["cvss_vector_std"] = raw[CONFIG["COLS"]["cvss"]].fillna("").apply(_standardize_cvss_vector)
raw["label"] = raw[CONFIG["COLS"]["label"]].apply(_to01)
raw = raw.dropna(subset=["label"]).copy()
raw["label"] = raw["label"].astype(int)
if CONFIG.get("DROP_IF_BOTH_EMPTY", True):
    mask = (raw["text_clean"].str.len() == 0) & (raw["cvss_vector_std"].str.len() == 0)
    raw = raw.loc[~mask].copy()
if not CONFIG.get("ALLOW_CVSS_INVALID", True):
    raw = raw[raw["cvss_vector_std"].str.len() > 0].copy()
prepared = raw[[CONFIG["COLS"]["id"], "text_clean", "cvss_vector_std", "label"]].rename(columns={CONFIG["COLS"]["id"]: "id", "text_clean": "description_clean"}).reset_index(drop=True)
prepared_path = OUT_DIR / "prepared_dataset.csv"
prepared.to_csv(prepared_path, index=False)
with open(OUT_DIR / "config.json", "w") as f:
    json.dump(CONFIG, f, indent=2)
if CONFIG.get("SAVE_CLASS_WEIGHTS", True):
    compute_and_save_class_weights(prepared["label"].values, OUT_DIR)

PAIR_RE = re2.compile(r"([A-Z]{1,3}):([A-Z])")
def parse_cvss(s):
    pairs = dict(PAIR_RE.findall(str(s)))
    return {k: pairs.get(k, "") for k in ["AV","AC","PR","UI","S","C","I","A"]}

df = prepared.copy()
parts = df["cvss_vector_std"].apply(parse_cvss).apply(pd.Series)
df = pd.concat([df, parts], axis=1)
AV_map = {"N":3, "A":2, "L":1, "P":0}
AC_map = {"L":1, "H":0}
PR_map = {"N":2, "L":1, "H":0}
UI_map = {"N":1, "R":0}
S_map  = {"U":0, "C":1}
IM_map = {"N":0, "L":1, "H":2}
df["AV_o"] = df["AV"].map(AV_map).fillna(-1).astype(int)
df["AC_o"] = df["AC"].map(AC_map).fillna(-1).astype(int)
df["PR_o"] = df["PR"].map(PR_map).fillna(-1).astype(int)
df["UI_o"] = df["UI"].map(UI_map).fillna(-1).astype(int)
df["S_o"]  = df["S"].map(S_map ).fillna(-1).astype(int)
df["C_o"]  = df["C"].map(IM_map).fillna(-1).astype(int)
df["I_o"]  = df["I"].map(IM_map).fillna(-1).astype(int)
df["A_o"]  = df["A"].map(IM_map).fillna(-1).astype(int)
df["impact_high_cnt"]  = (df[["C","I","A"]] == "H").sum(axis=1)
df["impact_low_cnt"]   = (df[["C","I","A"]] == "L").sum(axis=1)
df["impact_none_cnt"]  = (df[["C","I","A"]] == "N").sum(axis=1)
df["impact_composite"] = df["C_o"] + df["I_o"] + df["A_o"]
df["AVxPR_idx"]        = (df["AV_o"]+1)*10 + (df["PR_o"]+1)
categorical_cols = ["AV","AC","PR","UI","S","C","I","A"]
numeric_cols = ["AV_o","AC_o","PR_o","UI_o","S_o","C_o","I_o","A_o","impact_high_cnt","impact_low_cnt","impact_none_cnt","impact_composite","AVxPR_idx"]
fe_df = df[["id","label"] + categorical_cols + numeric_cols].copy()
fe_path = OUT_DIR / "fe_cvss.parquet"
fe_df.to_parquet(fe_path, index=False)
with open(OUT_DIR / "fe_meta.json","w") as f:
    json.dump({"categorical_cols": categorical_cols, "numeric_cols": numeric_cols, "n_rows": int(len(fe_df))}, f, indent=2)

df = pd.read_parquet(fe_path).reset_index(drop=True)
cat_core = ["AV","AC","PR","UI","S"]
for c in cat_core:
    vals = df[c].dropna().unique().tolist()
    for v in vals:
        if v == "": continue
        df[f"{c}_is_{v}"] = (df[c] == v).astype(int)
df["any_high_impact"] = ((df["C"] == "H") | (df["I"] == "H") | (df["A"] == "H")).astype(int)
df["all_high_impact"] = ((df["C"] == "H") & (df["I"] == "H") & (df["A"] == "H")).astype(int)
for c in ["AV","AC","PR","UI","S","C","I","A"]:
    df[f"is_missing_{c}"] = (df[c].isna() | (df[c].astype(str).str.len() == 0)).astype(int)
for c in ["AV_o","AC_o","PR_o","UI_o","S_o","C_o","I_o","A_o","impact_composite"]:
    if c not in df.columns: df[c] = np.nan
df["AVxPR_mul"] = (df["AV_o"].replace(-1, np.nan) * df["PR_o"].replace(-1, np.nan)).fillna(-1)
df["PR_minus_AV"] = (df["PR_o"].replace(-1, np.nan) - df["AV_o"].replace(-1, np.nan)).fillna(-999).astype(float)
df["impact_composite_sq"] = (df["impact_composite"].astype(float) ** 2).fillna(0)
df["PR_over_AV"] = np.where(df["AV_o"].replace(-1, np.nan).notna(), df["PR_o"].replace(-1, np.nan) / (df["AV_o"].replace(-1, np.nan) + 1e-6), -999.0)
for c in ["AV","PR"]:
    agg = df.groupby(c)["impact_composite"].agg(["mean","median","count"]).rename(columns={"mean":f"{c}_impact_mean","median":f"{c}_impact_median","count":f"{c}_count"})
    df = df.merge(agg, left_on=c, right_index=True, how="left")
TARGET_COL = "label"
N_FOLDS = 5
SMOOTH_ALPHA = 20.0
def target_encode_oof(data_df, by_col, target_col, n_folds=5, alpha=20.0, random_state=42):
    x = data_df[[by_col, target_col]].reset_index(drop=True).copy()
    result = np.zeros(len(x), dtype=float)
    prior = x[target_col].mean()
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    for tr_idx, va_idx in kf.split(x):
        tr = x.iloc[tr_idx]; va = x.iloc[va_idx]
        stats = tr.groupby(by_col)[target_col].agg(["sum","count"])
        stats["smoothed"] = (stats["sum"] + prior*alpha) / (stats["count"] + alpha)
        result[va_idx] = va[by_col].map(stats["smoothed"]).fillna(prior).values
    global_stats = x.groupby(by_col)[target_col].agg(["sum","count"])
    global_stats["smoothed"] = (global_stats["sum"] + prior*alpha) / (global_stats["count"] + alpha)
    return result, global_stats["smoothed"].to_dict()
te_cols = []
for c in ["AV","PR","AC"]:
    te_vals, mapping = target_encode_oof(df, by_col=c, target_col=TARGET_COL, n_folds=N_FOLDS, alpha=SMOOTH_ALPHA, random_state=42)
    df[f"te_{c}"] = te_vals
    df[f"{c}_te_global"] = df[c].map(mapping).fillna(df[TARGET_COL].mean())
    te_cols.append(f"te_{c}")
base_cols = ["id","label"] + categorical_cols + numeric_cols
other_cols = [c for c in df.columns if c not in base_cols and c not in ["id","label"]]
fe_enh = df[base_cols + other_cols].copy()
enh_path = OUT_DIR / "fe_cvss_enhanced.parquet"
fe_enh.to_parquet(enh_path, index=False)
meta_enh = {"base_categorical": categorical_cols, "base_numeric": numeric_cols, "added_features": other_cols, "n_rows": int(len(fe_enh)), "te_cols": te_cols, "te_params": {"n_folds": N_FOLDS, "alpha": SMOOTH_ALPHA}}
with open(OUT_DIR / "fe_meta_enhanced.json","w") as f:
    json.dump(meta_enh, f, indent=2)
dfc = pd.read_parquet(enh_path)
cat_base = ["AV","AC","PR","UI","S","C","I","A"]
num_base = ["AV_o","AC_o","PR_o","UI_o","S_o","C_o","I_o","A_o","impact_high_cnt","impact_low_cnt","impact_none_cnt","impact_composite","AVxPR_idx"]
base_cols_clean = ["id","label"] + cat_base + num_base
other_cols_clean = [c for c in dfc.columns if c not in base_cols_clean and c not in ["id","label"]]
final_cols = base_cols_clean + other_cols_clean
dfc = dfc[final_cols]
clean_path = OUT_DIR / "fe_cvss_enhanced_clean.parquet"
dfc.to_parquet(clean_path, index=False)
with open(OUT_DIR / "fe_meta_enhanced_clean.json","w") as f:
    json.dump({"base_categorical": cat_base, "base_numeric": num_base, "all_feature_cols": [c for c in dfc.columns if c not in ["id","label"]], "n_rows": int(len(dfc))}, f, indent=2)

import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score

device = "cuda" if torch.cuda.is_available() else "cpu"
PREP_DIR = Path("/content/prepared")
tab_df = pd.read_parquet(PREP_DIR / "fe_cvss.parquet").reset_index(drop=True)
meta = json.load(open(PREP_DIR / "fe_meta.json"))
cat_cols = meta["categorical_cols"]; num_cols = meta["numeric_cols"]

X_cat = tab_df[cat_cols].astype(object).fillna("NA")
cat_maps = {}
for c in cat_cols:
    cat_maps[c] = {cat:i for i,cat in enumerate(pd.Categorical(X_cat[c]).categories)}
    X_cat[c] = X_cat[c].map(cat_maps[c]).fillna(0).astype(int)
X_num = tab_df[num_cols].fillna(0).astype(float).values
y_tab = tab_df["label"].values

idx = np.arange(len(tab_df))
train_idx, val_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=y_tab)
Xcat_tr, Xcat_va = X_cat.values[train_idx], X_cat.values[val_idx]
Xnum_tr, Xnum_va = X_num[train_idx], X_num[val_idx]
ytr_tab, yva_tab = y_tab[train_idx], y_tab[val_idx]

BATCH = 256
Xtr_cat_t = torch.tensor(Xcat_tr, dtype=torch.long)
Xva_cat_t = torch.tensor(Xcat_va, dtype=torch.long)
Xtr_num_t = torch.tensor(Xnum_tr, dtype=torch.float32)
Xva_num_t = torch.tensor(Xnum_va, dtype=torch.float32)
ytr_t = torch.tensor(ytr_tab, dtype=torch.float32)
yva_t = torch.tensor(yva_tab, dtype=torch.float32)
tr_loader = DataLoader(TensorDataset(Xtr_cat_t, Xtr_num_t, ytr_t), batch_size=BATCH, shuffle=True)
va_loader = DataLoader(TensorDataset(Xva_cat_t, Xva_num_t, yva_t), batch_size=512)

class TabTransformerModel(nn.Module):
    def __init__(self, num_categories, num_numeric, emb_dim=32, hidden=128, n_layers=2, nheads=4):
        super().__init__()
        self.embs = nn.ModuleList([nn.Embedding(nc+1, emb_dim) for nc in num_categories])
        self.num_proj = nn.Linear(num_numeric, emb_dim)
        enc_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nheads, dim_feedforward=hidden, batch_first=True)
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.head = nn.Sequential(nn.Flatten(), nn.Linear(emb_dim*(len(num_categories)+1), hidden), nn.ReLU(), nn.Dropout(0.2))
        self.out  = nn.Sequential(nn.Linear(hidden,1), nn.Sigmoid())
    def forward(self, xcat, xnum):
        cs = [emb(xcat[:,i]) for i,emb in enumerate(self.embs)]
        cat_stack = torch.stack(cs, dim=1)
        num_tok = self.num_proj(xnum).unsqueeze(1)
        x = torch.cat([cat_stack, num_tok], dim=1)
        x_enc = self.encoder(x)
        feat = self.head(x_enc)
        out = self.out(feat).squeeze(-1)
        return out, feat

num_categories = [len(cat_maps[c]) for c in cat_cols]
model_tab = TabTransformerModel(num_categories, X_num.shape[1], emb_dim=32, hidden=128, n_layers=2).to(device)
opt = torch.optim.Adam(model_tab.parameters(), lr=1e-3)
loss_fn = nn.BCELoss()
EPOCHS = 8
for ep in range(EPOCHS):
    model_tab.train()
    total_loss = 0
    for xcat, xnum, y in tr_loader:
        xcat, xnum, y = xcat.to(device), xnum.to(device), y.to(device)
        opt.zero_grad()
        p, _ = model_tab(xcat, xnum)
        loss = loss_fn(p, y)
        loss.backward(); opt.step()
        total_loss += loss.item() * len(y)
    tr_loss = total_loss / len(tr_loader.dataset)
    model_tab.eval()
    preds = []
    with torch.no_grad():
        for xcat, xnum, y in va_loader:
            p, _ = model_tab(xcat.to(device), xnum.to(device))
            preds.extend(p.cpu().numpy())
    preds = np.array(preds)
    roc = roc_auc_score(yva_tab, preds)
    prec, rec, _ = precision_recall_curve(yva_tab, preds); pr = auc(rec, prec)
    f1 = f1_score(yva_tab, preds > 0.5)
    print(f"Tab Ep{ep+1}: loss={tr_loss:.4f} ROC={roc:.3f} PR={pr:.3f} F1={f1:.3f}")

torch.save(model_tab.state_dict(), PREP_DIR / "tab_transformer.pt")
import joblib
joblib.dump(cat_maps, PREP_DIR / "tab_cat_maps.joblib")

from sentence_transformers import SentenceTransformer
from sklearn.metrics import average_precision_score

df = pd.read_parquet(str(PREP_DIR / "fe_cvss.parquet"))
desc = pd.read_csv(str(PREP_DIR / "prepared_dataset.csv")).set_index("id")["description_clean"]
df = df.merge(desc.reset_index(), left_on="id", right_on="id", how="left")
if not os.path.exists(str(PREP_DIR / "text_embeddings.joblib")):
    txt_model = SentenceTransformer("all-MiniLM-L6-v2")
    texts = df["description_clean"].fillna("").astype(str).tolist()
    embs = txt_model.encode(texts, batch_size=128, show_progress_bar=True)
    joblib.dump(embs, str(PREP_DIR / "text_embeddings.joblib"))
text_embs = joblib.load(str(PREP_DIR / "text_embeddings.joblib"))
y = df["label"].values
idx = np.arange(len(df))
tr_idx, va_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=y)
Xtr_text, Xva_text = text_embs[tr_idx], text_embs[va_idx]
ytr, yva = y[tr_idx], y[va_idx]

class TextMLP(nn.Module):
    def __init__(self, in_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(0.2), nn.Linear(hidden,1), nn.Sigmoid())
    def forward(self,x): return self.net(x).squeeze(-1)

model_text = TextMLP(Xtr_text.shape[1], hidden=128).to(device)
opt = torch.optim.Adam(model_text.parameters(), lr=1e-3)
loss_fn = nn.BCELoss()
tr_loader = DataLoader(TensorDataset(torch.tensor(Xtr_text,dtype=torch.float32), torch.tensor(ytr,dtype=torch.float32)), batch_size=256, shuffle=True)
va_loader = DataLoader(TensorDataset(torch.tensor(Xva_text,dtype=torch.float32), torch.tensor(yva,dtype=torch.float32)), batch_size=256)
for ep in range(8):
    model_text.train(); tot=0
    for xb,yb in tr_loader:
        xb,yb = xb.to(device), yb.to(device)
        opt.zero_grad(); p = model_text(xb); l = loss_fn(p,yb); l.backward(); opt.step()
        tot += l.item()*len(yb)
    model_text.eval(); preds=[]
    with torch.no_grad():
        for xb,yb in va_loader:
            preds.extend(model_text(xb.to(device)).cpu().numpy())
    preds = np.array(preds)
    roc = roc_auc_score(yva, preds)
    prec, rec, _ = precision_recall_curve(yva, preds); pr = auc(rec, prec)
    f1 = f1_score(yva, preds>0.5)
    print(f"Text Ep{ep+1}: loss={tot/len(tr_loader.dataset):.4f} ROC={roc:.3f} PR={pr:.3f} F1={f1:.3f}")
torch.save(model_text.state_dict(), str(PREP_DIR / "text_mlp.pt"))

from sklearn.linear_model import LogisticRegression

tab_df = pd.read_parquet(f"{PREP_DIR}/fe_cvss.parquet").reset_index(drop=True)
desc = pd.read_csv(f"{PREP_DIR}/prepared_dataset.csv").set_index("id")["description_clean"]
tab_df = tab_df.merge(desc.reset_index(), on="id", how="left")
y = tab_df["label"].values
meta = json.load(open(f"{PREP_DIR}/fe_meta.json"))
cat_cols, num_cols = meta["categorical_cols"], meta["numeric_cols"]
cat_maps = joblib.load(f"{PREP_DIR}/tab_cat_maps.joblib")
X_cat = tab_df[cat_cols].astype(object).fillna("NA")
for c in cat_cols:
    X_cat[c] = X_cat[c].map(cat_maps[c]).fillna(0).astype(int)
X_num = tab_df[num_cols].fillna(0).astype(float).values
idx = np.arange(len(tab_df))
tr_idx, va_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=y)
y_tr, y_va = y[tr_idx], y[va_idx]

class TabTransformerModel(nn.Module):
    def __init__(self, num_categories, num_numeric, emb_dim=32, hidden=128, n_layers=2, nheads=4):
        super().__init__()
        self.embs = nn.ModuleList([nn.Embedding(nc+1, emb_dim) for nc in num_categories])
        self.num_proj = nn.Linear(num_numeric, emb_dim)
        enc = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nheads, dim_feedforward=hidden, batch_first=True)
        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)
        self.head = nn.Sequential(nn.Flatten(), nn.Linear(emb_dim*(len(num_categories)+1), hidden), nn.ReLU(), nn.Dropout(0.2))
        self.out  = nn.Sequential(nn.Linear(hidden,1), nn.Sigmoid())
    def forward(self, xcat, xnum):
        cs = [emb(xcat[:,i]) for i,emb in enumerate(self.embs)]
        cat_stack = torch.stack(cs, dim=1)
        num_tok = self.num_proj(xnum).unsqueeze(1)
        x = torch.cat([cat_stack, num_tok], dim=1)
        x_enc = self.encoder(x)
        feat = self.head(x_enc)
        out = self.out(feat).squeeze(-1)
        return out, feat

num_categories = [len(cat_maps[c]) for c in cat_cols]
model_tab = TabTransformerModel(num_categories, X_num.shape[1]).to(device)
model_tab.load_state_dict(torch.load(f"{PREP_DIR}/tab_transformer.pt", map_location=device))
model_tab.eval()
with torch.no_grad():
    xcat_tr = torch.tensor(X_cat.values[tr_idx], dtype=torch.long).to(device)
    xnum_tr = torch.tensor(X_num[tr_idx], dtype=torch.float32).to(device)
    p_tab_tr, _ = model_tab(xcat_tr, xnum_tr); p_tab_tr = p_tab_tr.cpu().numpy()
    xcat_va = torch.tensor(X_cat.values[va_idx], dtype=torch.long).to(device)
    xnum_va = torch.tensor(X_num[va_idx], dtype=torch.float32).to(device)
    p_tab_va, _ = model_tab(xcat_va, xnum_va); p_tab_va = p_tab_va.cpu().numpy()

text_embs = joblib.load(f"{PREP_DIR}/text_embeddings.joblib")
in_dim = text_embs.shape[1]
class TextMLP(nn.Module):
    def __init__(self, in_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(0.2), nn.Linear(hidden,1), nn.Sigmoid())
    def forward(self,x): return self.net(x).squeeze(-1)
model_text = TextMLP(in_dim, hidden=128).to(device)
model_text.load_state_dict(torch.load(f"{PREP_DIR}/text_mlp.pt", map_location=device))
model_text.eval()
with torch.no_grad():
    p_txt_tr = model_text(torch.tensor(text_embs[tr_idx], dtype=torch.float32).to(device)).cpu().numpy()
    p_txt_va = model_text(torch.tensor(text_embs[va_idx], dtype=torch.float32).to(device)).cpu().numpy()

def report_metrics(name, y_true, scores):
    roc = roc_auc_score(y_true, scores)
    prec, rec, _ = precision_recall_curve(y_true, scores)
    pr_auc = auc(rec, prec)
    ap = average_precision_score(y_true, scores)
    f1 = f1_score(y_true, scores>0.5)
    print(f"{name:>14} | ROC={roc:.3f}  PR-AUC={pr_auc:.3f}  AP={ap:.3f}  F1@0.5={f1:.3f}")
    return {"roc":roc,"pr":pr_auc,"ap":ap,"f1":f1}

print("\n== Single-modal metrics (val) ==")
report_metrics("Tabular (val)", y_va, p_tab_va)
report_metrics("Text    (val)", y_va, p_txt_va)
p_avg_va = 0.5*(p_tab_va + p_txt_va)
report_metrics("AvgFuse (val)", y_va, p_avg_va)
stack_tr = np.vstack([p_tab_tr, p_txt_tr]).T
stack_va = np.vstack([p_tab_va, p_txt_va]).T
meta_lr = LogisticRegression().fit(stack_tr, y_tr)
p_meta_va = meta_lr.predict_proba(stack_va)[:,1]
report_metrics("LogitFuse(val)", y_va, p_meta_va)
print(meta_lr.coef_.ravel(), meta_lr.intercept_)